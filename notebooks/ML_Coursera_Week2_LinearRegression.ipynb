{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Coursera_Week2_LinearRegression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4eYO2IPf7grx0qQ+cpVLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/ML/blob/main/notebooks/ML_Coursera_Week2_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMKloU0lRTCG"
      },
      "source": [
        "# Week2\n",
        "\n",
        "## Quiz: Linear Regression with Multiple Variables.\n",
        "1. What is the normalized feature $ x_2^{(2)}$? (Hint: midterm = 72, final = 74 is training example 2.)\n",
        "\n",
        "| midterm exam | (midterm exam)^2 | final exam |\n",
        "|--- | --- | --- |\n",
        "89 | 7921 | 96 |\n",
        "72 | 5184 |\t74\n",
        "94 |\t8836 |\t87\n",
        "69 |\t4761 |\t78\n",
        "\n",
        "\n",
        "To calculate the normalized feature:\n",
        "\n",
        "$ xi := \\Large \\frac{x_i - \\mu_i}{s_i} $\n",
        "\n",
        "where $ \\mu_i $ is the mean and $s_i$ is the range (max-min).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLp8KRdTRKrt",
        "outputId": "864701ca-3b2f-4b03-84c3-fd3693d67848"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x0 = np.array([89, 72, 94, 69])\n",
        "x1 = np.array([7921, 5184, 8836, 4761])\n",
        "x2 = np.array([96, 74, 87, 78])\n",
        "\n",
        "mu1 = x1.mean()\n",
        "mu1\n",
        "\n",
        "s1 = x1.max() - x1.min()\n",
        "s1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4075"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WHrZUucT7gK",
        "outputId": "ed4e36c3-0f8b-4aa8-e68c-20260f4a825f"
      },
      "source": [
        "x1n = (x1[1] - mu1)/s1\n",
        "x1n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3660122699386503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoY6ss9_Z0RX"
      },
      "source": [
        "## Question 2\n",
        "You run gradient descent for 15 iterations with $ \\alpha = 0.3$ and compute $ J(\\theta)$ after each iteration. You find that the value of $ J(\\theta) $ decreases slowly and is still decreasing after 15 iterations. Based on this, which of the following conclusions seems most plausible?\n",
        "\n",
        "- Rather than use the current value of $\\alpha$, it'd be more promising to try a larger value of $\\alpha$ (say $\\alpha = 1.0$). \n",
        "\n",
        "\n",
        "### version 2\n",
        "value of J(\\theta)J(θ) decreases quickly then levels off. Based on this, which of the following conclusions seems most plausible? \n",
        "\n",
        "- $\\alpha = 0.3$ is an effective choice of learning rate. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqBCI1UYVG7D"
      },
      "source": [
        "## Question 3 \n",
        "Suppose you have m = 14 training examples with n = 3 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is $ \\theta = (X^TX)^{-1}X^TY$. For the given values of m and n, what are the dimensions of $\\theta$, X, and y in this equation?\n",
        "\n",
        "- X is 14×4, y is 14×1, $\\theta$ is 4×1 \n",
        "\n",
        "\n",
        " Suppose you have m = 28 training examples with n = 4 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is $\\theta = (X^TX)^{-1}X^Ty$. For the given values of mm and nn, what are the dimensions of $\\theta$, X, and y in this equation?\n",
        " - X is 28x4, y is 28×1, θ is 4×1 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O8faDrSbQnT"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Which of the following are reasons for using feature scaling?\n",
        "-  It speeds up gradient descent by making each iteration of gradient descent less expensive to compute. \n",
        "- NO\n",
        "\n",
        " It speeds up gradient descent by making it require fewer iterations to get to a good solution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SJGp5A4UNzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
